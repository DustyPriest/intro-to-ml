{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ffa401",
   "metadata": {},
   "source": [
    "# Assignment 2: Classification and Evaluation (20 marks)\n",
    "\n",
    "Student Name: `Please enter you full name here`\n",
    "\n",
    "Student ID: `Please enter you student number here`\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: *5 pm on Friday 7th of April*\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -2;</li>\n",
    "    <li>two days late, -4;</li>\n",
    "    <li>three days late, -6;</li>\n",
    "    <li>four days late, -8;</li>\n",
    "    <li>five days late, -10;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 20, and make up 20% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page] on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages `numpy`, `pandas`, `matplotlib` and `sklearn`. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on Canvas>Assignments>Assignmnet1; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: This assignment is an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where collusion or plagiarism are deemed to have taken place. Content produced by an AI (including, but not limited to ChatGPT) is not your own work, and submitting such content will be treated as a case of academic misconduct, in line with the <a href=\"https://academicintegrity.unimelb.edu.au/plagiarism-and-collusion/artificial-intelligence-tools-and-technologies\"> University's policy</a>.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>Missing Authorship Declaration at the bottom of the page, -2.0\n",
    "<LI>Incomplete or unsigned Authorship Declaration at the bottom of the page, -1.0\n",
    "</UL>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d47f4",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "For this assignment, you will work with a provided dataset to train and utilize multiple classifiers to analyze different versions of the dataset. In addition to implementing these classifiers, you will also explore various evaluation paradigms and analyze the impact of multiple parameters on the performance of the classifiers. Finally, you will be expected to answer some conceptual questions based on your observations and analysis.\n",
    "\n",
    "## Data Set:\n",
    "In this assignment, you will work with multiple versions of one dataset called \"Amphibians.\" It is adopted from a famous public dataset, and you can find more details about it <a href= \"https://archive.ics.uci.edu/ml/datasets/Amphibians#\"> here </a>.  The dataset includes information about five groups of amphibians in Poland: \"Green frogs,\" \"Brown frogs,\" \"Common toad,\" \"Tree frog,\" and \"Fire-bellied toad.\" The dataset comprises 14 attributes and one class. Some of these attributes are numeric, some are categorical, and some are ordinal.\n",
    "\n",
    "You can find details about all the features in the dataset in the file \"README.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa339cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90b111-3c9f-4b4c-a0a6-413000636fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# ignore future warnings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1c496",
   "metadata": {},
   "source": [
    "## Question 1 [5 marks]\n",
    "**Q1.1 A.** Read the dataset \"amphibians1.csv\" dataset into a pandas DataFrame called `data1`. Create a function called `encode` that takes in the features of the dataset as a pandas DataFrame and uses one-hot encoding to convert all nominal (and ordinal) attributes to numeric. You can achieve this by either using `get_dummies()` from the pandas library or `OneHotEncoder()` from the scikit-learn library. **[1 mark]**\n",
    "\n",
    "**B.** For 10 rounds, use `train_test_split` to divide the encoded `data1` into 70% train, 30% test . Set the `random_state` equal to the loop counter. For example in the loop\n",
    "``` python \n",
    "for i in range(10):\n",
    "```\n",
    "make `random_state` equal to `i`. \n",
    "Use the splitted datasets to train and test the following models (use the default hyperparameters): **[1 mark]**\n",
    "- Zero-R\n",
    "- Gaussian Naive Bayes\n",
    "- Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes model\n",
    "\n",
    "Report the average accuracy over the 10 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"amphibians1.csv\")\n",
    "\n",
    "y1 = data1.iloc[:, -1]\n",
    "X1 = data1.iloc[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588459ef-a861-4793-8c92-2f57bab4bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "def encode(X):\n",
    "    # your code here\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3d435-af7a-4bd4-b481-f22b1c7550a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroR_Acc_1 = []\n",
    "GNB_Acc_1 = []\n",
    "MNB_Acc_1 = []\n",
    "BNB_Acc_1 = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy of ZeroR:\", np.mean(ZeroR_Acc_1).round(2))\n",
    "print(\"Accuracy of GNB:\", np.mean(GNB_Acc_1).round(2))\n",
    "print(\"Accuracy of MNB:\", np.mean(MNB_Acc_1).round(2))\n",
    "print(\"Accuracy of BNB:\", np.mean(BNB_Acc_1).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c394c5",
   "metadata": {},
   "source": [
    "**Q1.2** After comparing the performance of the different models on the classification task, please comment on any differences or lack of differences you observe between the models. **[3 marks]**</br>\n",
    "*NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3936c2-b548-4825-8028-c5b3f3800b63",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec6a7b",
   "metadata": {},
   "source": [
    "## Question 2 [5 marks]\n",
    "\n",
    "**Q2.1.** Divide the `data1` into 70% train and 30% test splits for 10 rounds, set the `random_state` equal to the loop counter. Then, train and test **K-Nearest Neighbor algorithms (with K values of 1, 5, and 20)**, using Euclidean distance as the distance metric and maximum vote (no weighting) to determine the label. Finally, report the average accuracy of the KNN models over the 10 runs. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef637cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN1_Acc_1 = []\n",
    "KNN5_Acc_1 = []\n",
    "KNN20_Acc_1 = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "print(\"Accuracy of KNN(1):\", np.mean(KNN1_Acc_1).round(2))\n",
    "print(\"Accuracy of KNN(5):\", np.mean(KNN5_Acc_1).round(2))\n",
    "print(\"Accuracy of KNN(20):\", np.mean(KNN20_Acc_1).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4822d",
   "metadata": {},
   "source": [
    "**Q2.2.A.** Create a function called `normalise` that takes the features of the dataset as a pandas DataFrame and scales all numeric attributes to the range of 0-1. You can either use `MinMaxScaler` from the `sklearn.preprocessing` library or implement the normalization step yourself.\n",
    "**B.** For 10 rounds divide the normalised \"data1\" into 70% train and 30% test splits using set the `random_state` equal to the loop counter, and run the KNN models (k=1,5 and 20). Report the average accuracy of your KNN models over these 10 runs. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e87f9-76bf-44e2-a7bc-305bdcd1616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "def normalise(X):\n",
    "    # your code here\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## POSSIBLE SOLUTION #############################################\n",
    "\n",
    "KNN1_Acc_2 = []\n",
    "KNN5_Acc_2 = []\n",
    "KNN20_Acc_2 = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "print(\"Accuracy of KNN(1):\", np.mean(KNN1_Acc_2).round(2))\n",
    "print(\"Accuracy of KNN(5):\", np.mean(KNN5_Acc_2).round(2))\n",
    "print(\"Accuracy of KNN(20):\", np.mean(KNN20_Acc_2).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357cd59",
   "metadata": {},
   "source": [
    "**Q2.3** Compare the results of the KNN models (for each value of K) in Q2.1 and Q2.2, and discuss any differences you observe. Did the preprocessing step in Q2.2 improve the performance of the KNN models? Why or why not? **[3 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc7597",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533050f",
   "metadata": {},
   "source": [
    "## Question 3 [5 marks]\n",
    "\n",
    "**Q3.1.** Read the \"amphibians2.csv\" into `data2` and  \"amphibians1.csv\" into `data1_2`. Observe any differences in the values of the feature \"SR\" between these two datasets, and explain why you think these changes were made (provide your hypothesis). Use diagrams such a histograms or boxplots to check the distribution of the feature 'SR' before and after the change. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf383bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"amphibians2.csv\")\n",
    "data1_2 = pd.read_csv(\"amphibians1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93740fd5-3ec4-4b50-8d0c-def8367fa0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfddfd-cc6b-47d7-9d26-a3d699ac5d10",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec583024",
   "metadata": {},
   "source": [
    "**Q3.2** \n",
    "For 10 rounds plit the `data2` and `data1_2` dataset into 70% training and 30% testing sets use the `random_state` equal to the loop counter. Train and test two **decision tree** classifier with all default hyper-parameters: one with `data1_2` and one with `data2`. Calculate and report the average accuracy of both models for 10 runs. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca15dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_2 = data1_2.iloc[:, -1]\n",
    "X1_2 =  data1_2.iloc[:, :-1]\n",
    "\n",
    "y2 = data2.iloc[:, -1]\n",
    "X2 = data2.iloc[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18064b7e-be1e-40ba-9197-100b8c236ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_Acc_1_2 = []\n",
    "DT_Acc_2 = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy of Decision Tree \\tdata1_2:\", np.mean(DT_Acc_1_2).round(2),\"\\tdata2:\", np.mean(DT_Acc_2).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dd7fb",
   "metadata": {},
   "source": [
    "**Q3.3** Compare and analyze the performance of the **decision tree** classifier on `data1_2` and `data2`. Discuss any differences or similarities that you observe in the performance of these models. Does the change made to the dataset improve the performance of the model? Explain why or why not and elaborate on your hypothesis from Q3.1. **[2 marks]** </br>*NOTE:  You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e6985",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2419db",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**Q4.1** Read the \"amphibians3.csv\" into data3. Use histogram diagrams to compare the distribution of class labels between `data1` and `data3`. Observe the changes on the distribution of the class labels and explain in your own words why you think these changes have been made.**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv(\"amphibians3.csv\")\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81f752",
   "metadata": {},
   "source": [
    "**Q4.2** Use the same encoding technique as used in Q1.1 to encode the `data3` dataset and apply the same normalization technique as used in Q2.1 to normalise the data. For 10 rounds, split the encoded and normalized data3 into 70% training and 30% testing sets,  set the `random_state` equal to the loop counter. Train and test the following models using `data3`: **[1 mark]**\n",
    "- Zero-R\n",
    "- K-Nearest Neighbour (K = 1, 5 and 20) \n",
    "\n",
    "Calculate the average accuracy of the models for 10 runs and report the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = data3.iloc[:, -1]\n",
    "X3 = data3.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fad1b-d2fc-4adc-818b-9cee1c204bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroR_Acc_3 = []\n",
    "KNN1_Acc_3 = []\n",
    "KNN5_Acc_3 = []\n",
    "KNN20_Acc_3 = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy of ZeroR: \\tdata1:\", np.mean(ZeroR_Acc_1).round(2),\"\\tdata3:\",np.mean(ZeroR_Acc_3).round(2))\n",
    "print(\"Accuracy of KNN(1): \\tdata1:\", np.mean(KNN1_Acc_2).round(2),\"\\tdata3:\", np.mean(KNN1_Acc_3).round(2))\n",
    "print(\"Accuracy of KNN(5): \\tdata1:\", np.mean(KNN5_Acc_2).round(2),\"\\tdata3:\", np.mean(KNN5_Acc_3).round(2))\n",
    "print(\"Accuracy of KNN(20): \\tdata1:\", np.mean(KNN20_Acc_2).round(2),\"\\tdata3:\", np.mean(KNN20_Acc_3).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4249859",
   "metadata": {},
   "source": [
    "**Q4.3** Discuss any differences you observe between the results of these KNN classifiers on data1 (Q2.2) and data3. **[2 marks]** </br>*NOTE:  You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067969a",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55940e72-3414-4dab-be69-96a579354617",
   "metadata": {},
   "source": [
    "# Authorship Declaration:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: [Enter your full name and student number here before submission]\n",
    "   \n",
    "   <b>Dated</b>: [Enter the date that you \"signed\" the declaration]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
